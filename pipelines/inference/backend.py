import importlib
import json
import logging
import os
import random
import re
import sqlite3
import uuid
from abc import ABC, abstractmethod
from datetime import datetime, timezone
from pathlib import Path
import tempfile

import numpy as np
import pandas as pd
import torch
import tifffile


def percentile_normalization(image, pmin=2, pmax=99.8, axis=None):
    """
    Compute a percentile normalization for the given image.
    
    Parameters:
    - image (array): array (2D or 3D) of the image file.
    - pmin (int or float): the minimal percentage for the percentiles to compute.
                           Values must be between 0 and 100 inclusive.
    - pmax (int or float): the maximal percentage for the percentiles to compute.
                           Values must be between 0 and 100 inclusive.
    - axis: Axis or axes along which the percentiles are computed.
            The default (=None) is to compute it along a flattened version of the array.

    Returns:
    Normalized image (np.ndarray): An array containing the normalized image.
    """
    if not (np.isscalar(pmin) and np.isscalar(pmax) and 0 <= pmin < pmax <= 100):
        raise ValueError("Invalid values for pmin and pmax")

    low_percentile = np.percentile(image, pmin, axis=axis, keepdims=True)
    high_percentile = np.percentile(image, pmax, axis=axis, keepdims=True)

    if low_percentile == high_percentile:
        logging.warning(f"Same min {low_percentile} and high {high_percentile}, image may be empty")
        return image

    return (image - low_percentile) / (high_percentile - low_percentile)


def compute_3d_ssim(img1, img2):
    """
    Compute SSIM between two 3D volumes.
    
    Args:
        img1 (numpy.ndarray): First 3D volume
        img2 (numpy.ndarray): Second 3D volume
        
    Returns:
        float: SSIM score
    """
    from skimage.metrics import structural_similarity as ssim
    
    # Ensure both images are normalized
    img1 = percentile_normalization(img1)
    img2 = percentile_normalization(img2)
    
    # Compute SSIM for each slice and average
    ssim_values = []
    for z in range(img1.shape[0]):
        ssim_slice = ssim(img1[z], img2[z], data_range=1.0)
        ssim_values.append(ssim_slice)
    
    return np.mean(ssim_values)


def compute_n_ssim(prediction, ground_truth, input_image):
    """
    Compute Normalized SSIM as defined in the challenge.
    
    Args:
        prediction (numpy.ndarray): Predicted 3D volume
        ground_truth (numpy.ndarray): Ground truth 3D volume
        input_image (numpy.ndarray): Input 3D volume (single view)
        
    Returns:
        float: Normalized SSIM score
    """
    # Compute SSIM between prediction and ground truth
    prediction_ssim = compute_3d_ssim(prediction, ground_truth)
    
    # Compute reference SSIM between input and ground truth
    reference_ssim = compute_3d_ssim(input_image, ground_truth)
    
    # Compute normalized SSIM
    n_ssim = (prediction_ssim - reference_ssim) / (1 - reference_ssim)
    
    return n_ssim


def predict_fused_view(model, single_view):
    """
    Generate a fused multi-view prediction from a single view input.
    
    Args:
        model: PyTorch model
        single_view: 3D numpy array (Z, Y, X)
        
    Returns:
        fused_view: 3D numpy array (Z, Y, X)
    """
    # Ensure model is in evaluation mode
    model.eval()
    
    # Normalize input
    normalized_input = percentile_normalization(single_view)
    
    # Convert to tensor and add batch and channel dimensions
    input_tensor = torch.from_numpy(normalized_input).float().unsqueeze(0).unsqueeze(0)
    
    # Move to the same device as the model
    device = next(model.parameters()).device
    input_tensor = input_tensor.to(device)
    
    # Generate prediction
    with torch.no_grad():
        output_tensor = model(input_tensor)
    
    # Convert back to numpy
    fused_view = output_tensor.cpu().squeeze().numpy()
    
    return fused_view


class Backend(ABC):
    """Abstract class defining the interface of a backend."""

    @abstractmethod
    def load(self, limit: int) -> pd.DataFrame | None:
        """Load production data from the backend database.

        Args:
            limit: The maximum number of samples to load from the database.

        """

    @abstractmethod
    def save(self, model_input: pd.DataFrame, model_output: list) -> None:
        """Save production data and model outputs to the database.

        Args:
            model_input: The input data received by the model
            model_output: The output data generated by the model.

        """

    @abstractmethod
    def label(self, ground_truth_quality: float = 0.8) -> int:
        """Label every unlabeled sample stored in the backend database.

        This function will generate fake ground truth data for any unlabeled samples
        stored in the backend database.

        Args:
            ground_truth_quality: The quality of the ground truth labels to generate.
                A value of 1.0 will generate labels that match the model predictions. A
                value less than 1.0 will introduce noise in the labels to simulate
                inaccurate model predictions

        """

    @abstractmethod
    def invoke(self, payload: list | dict) -> dict | None:
        """Make a prediction request to the hosted model.

        Args:
            payload: The data to send to the model for prediction.

        """

    @abstractmethod
    def deploy(self, model_uri: str, model_version: str) -> None:
        """Deploy the supplied model.

        Args:
            model_uri: The path where the model artifacts are located.
            model_version: The version of the model that will be deployed.

        """

    def get_fake_label(self, prediction, ground_truth_quality):
        """Generate a fake ground truth label for a sample.

        This function will randomly return a ground truth label taking into account the
        prediction quality we want to achieve.

        Args:
            prediction: The model prediction for the sample.
            ground_truth_quality: The quality of the ground truth labels to generate.

        """
        # For FuseMyCell, we need to create a fake ground truth 3D volume
        # This is a simplified implementation - in a real scenario, you would
        # want to generate more realistic synthetic ground truth
        
        # Get the shape of the prediction
        if hasattr(prediction, 'shape'):
            shape = prediction.shape
        else:
            # If prediction is not a numpy array, use a default shape
            shape = (64, 128, 128)  # Default patch size
            
        # Create random noise with similar statistics as the prediction
        noise = np.random.normal(0.5, 0.2, shape)
        
        # Blend prediction with noise based on quality
        if ground_truth_quality >= 1.0:
            # Return exact prediction (perfect quality)
            return prediction
        else:
            # Blend prediction with noise
            blend = ground_truth_quality * prediction + (1 - ground_truth_quality) * noise
            
            # Normalize to [0, 1] range
            return percentile_normalization(blend)


class Local(Backend):
    """Local backend implementation.

    A model with this backend will be deployed using `mlflow model serve` and will use
    a SQLite database to store production data.
    """

    def __init__(self, config: dict | None = None) -> None:
        """Initialize backend using the supplied configuration.

        If the configuration is not provided, the class will attempt to read the
        configuration from environment variables.
        """
        self.target = (
            config.get("target", "http://127.0.0.1:8080/invocations")
            if config
            else "http://127.0.0.1:8080/invocations"
        )
        self.database = "fusemycell.db"

        if config:
            self.database = config.get("database", self.database)
        else:
            self.database = os.getenv("MODEL_BACKEND_DATABASE", self.database)
            
        # Data directory for storing TIFF files
        self.data_dir = os.path.join(os.path.dirname(self.database), "tiff_data")
        os.makedirs(self.data_dir, exist_ok=True)

        logging.info("Backend database: %s", self.database)
        logging.info("TIFF data directory: %s", self.data_dir)

    def load(self, limit: int = 100) -> pd.DataFrame | None:
        """Load production data from a SQLite database."""
        if not Path(self.database).exists():
            logging.error("Database %s does not exist.", self.database)
            return None

        connection = sqlite3.connect(self.database)

        query = (
            "SELECT uuid, input_path, output_path, n_ssim, ground_truth_path, prediction_date "
            "FROM predictions "
            "ORDER BY prediction_date DESC LIMIT ?;"
        )

        data = pd.read_sql_query(query, connection, params=(limit,))
        connection.close()

        return data

    def save(self, model_input: list | dict, model_output: list) -> None:
        """Save production data to a SQLite database.

        If the database doesn't exist, this function will create it.
        """
        logging.info("Storing production data in the database...")

        # Ensure model_input is a list
        if isinstance(model_input, dict):
            model_input = [model_input]

        connection = None
        try:
            connection = sqlite3.connect(self.database)
            
            # Create the table if it doesn't exist
            connection.execute(
                """
                CREATE TABLE IF NOT EXISTS predictions (
                    uuid TEXT PRIMARY KEY,
                    input_path TEXT,
                    output_path TEXT,
                    n_ssim REAL,
                    ground_truth_path TEXT,
                    prediction_date TIMESTAMP
                )
                """
            )

            # Process and save each prediction
            current_time = datetime.now(timezone.utc).isoformat()
            
            for i, (input_data, output_data) in enumerate(zip(model_input, model_output)):
                record_uuid = str(uuid.uuid4())
                
                # Handle input data
                input_path = input_data.get('file_path')
                if not input_path:
                    # If no file path was provided, save the input data to a file
                    if 'data' in input_data and isinstance(input_data['data'], np.ndarray):
                        input_path = os.path.join(self.data_dir, f"{record_uuid}_input.tif")
                        tifffile.imwrite(input_path, input_data['data'])
                
                # Handle output data
                output_path = output_data.get('output_path')
                if output_path:
                    # Copy the output file to our data directory for long-term storage
                    new_output_path = os.path.join(self.data_dir, f"{record_uuid}_output.tif")
                    try:
                        # Read and then write the file (to avoid file system issues)
                        output_data = tifffile.imread(output_path)
                        tifffile.imwrite(new_output_path, output_data)
                        output_path = new_output_path
                    except Exception as e:
                        logging.error(f"Failed to copy output file: {e}")
                
                # Extract N-SSIM if available
                n_ssim = output_data.get('n_ssim')
                
                # Get ground truth path if available
                ground_truth_path = input_data.get('ground_truth_path')
                
                # Insert record into the database
                connection.execute(
                    """
                    INSERT INTO predictions 
                    (uuid, input_path, output_path, n_ssim, ground_truth_path, prediction_date)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (record_uuid, input_path, output_path, n_ssim, ground_truth_path, current_time)
                )
            
            connection.commit()
            logging.info(f"Saved {len(model_output)} prediction records to database")

        except sqlite3.Error:
            logging.exception(
                "There was an error saving production data to the database.",
            )
        finally:
            if connection:
                connection.close()

    def label(self, ground_truth_quality: float = 0.8) -> int:
        """Label unlabeled samples by generating synthetic ground truth."""
        if not Path(self.database).exists():
            logging.error("Database %s does not exist.", self.database)
            return 0

        connection = None
        try:
            connection = sqlite3.connect(self.database)

            # Get all records without ground truth
            df = pd.read_sql_query(
                "SELECT * FROM predictions WHERE ground_truth_path IS NULL",
                connection,
            )
            logging.info("Loaded %s unlabeled samples.", len(df))

            # If there are no unlabeled samples, we don't need to do anything else.
            if df.empty:
                return 0

            labeled_count = 0
            for _, row in df.iterrows():
                uuid_val = row["uuid"]
                output_path = row["output_path"]
                
                if not output_path or not os.path.exists(output_path):
                    continue
                
                try:
                    # Load the prediction
                    prediction = tifffile.imread(output_path)
                    
                    # Generate synthetic ground truth
                    ground_truth = self.get_fake_label(prediction, ground_truth_quality)
                    
                    # Save the synthetic ground truth
                    ground_truth_path = os.path.join(self.data_dir, f"{uuid_val}_ground_truth.tif")
                    tifffile.imwrite(ground_truth_path, ground_truth)
                    
                    # Update the database
                    connection.execute(
                        "UPDATE predictions SET ground_truth_path = ? WHERE uuid = ?",
                        (ground_truth_path, uuid_val)
                    )
                    
                    labeled_count += 1
                except Exception as e:
                    logging.error(f"Error labeling sample {uuid_val}: {e}")

            connection.commit()
            logging.info(f"Labeled {labeled_count} samples")
            return labeled_count
            
        except Exception:
            logging.exception("There was an error labeling production data")
            return 0
        finally:
            if connection:
                connection.close()

    def invoke(self, payload: list | dict) -> dict | None:
        """Make a prediction request to the hosted model."""
        import requests

        logging.info('Running prediction on "%s"...', self.target)

        try:
            predictions = requests.post(
                url=self.target,
                headers={"Content-Type": "application/json"},
                data=json.dumps(
                    {
                        "inputs": payload,
                    },
                ),
                timeout=5,
            )
            return predictions.json()
        except Exception:
            logging.exception("There was an error sending traffic to the endpoint.")
            return None

    def deploy(self, model_uri: str, model_version: str) -> None:
        """Not Implemented.

        Deploying a model is not applicable when serving the model directly.
        """
        logging.info(
            "Deploy not implemented for Local backend. "
            "Use 'mlflow models serve' to serve the model."
        )


class JsonLines(Backend):
    """JsonLines backend implementation.

    A model with this backend will store production data in a JSON Lines formatted file.
    Each line in the file is a valid JSON object representing an input request and its 
    corresponding prediction.
    """

    def __init__(self, config: dict | None = None) -> None:
        """Initialize backend using the supplied configuration.

        If the configuration is not provided, the class will attempt to read the
        configuration from environment variables.
        """
        self.target = (
            config.get("target", "http://127.0.0.1:8080/invocations")
            if config
            else "http://127.0.0.1:8080/invocations"
        )
        
        # File path for the JSON Lines storage
        self.storage_file = "fusemycell.jsonl"

        if config:
            self.storage_file = config.get("storage_file", self.storage_file)
        else:
            self.storage_file = os.getenv("MODEL_BACKEND_STORAGE_FILE", self.storage_file)

        # Data directory for storing TIFF files
        self.data_dir = os.path.join(os.path.dirname(self.storage_file), "tiff_data")
        os.makedirs(self.data_dir, exist_ok=True)
        
        # Make sure the directory exists
        storage_dir = os.path.dirname(self.storage_file)
        if storage_dir and not os.path.exists(storage_dir):
            os.makedirs(storage_dir)

        logging.info("Backend storage file: %s", self.storage_file)
        logging.info("TIFF data directory: %s", self.data_dir)

    def load(self, limit: int = 100) -> pd.DataFrame | None:
        """Load production data from the JSON Lines file."""
        if not Path(self.storage_file).exists():
            logging.error("Storage file %s does not exist.", self.storage_file)
            return None

        try:
            # Read the JSON Lines file into a DataFrame
            data = []
            with open(self.storage_file, "r") as f:
                for line in f:
                    data.append(json.loads(line))
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # If there are no records, return an empty DataFrame
            if df.empty:
                return df
                
            # Filter for records that have ground truth
            df_with_gt = df[df["ground_truth_path"].notna()]
            
            # Return the most recent 'limit' records
            return df_with_gt.sort_values("prediction_date", ascending=False).head(limit)
            
        except Exception:
            logging.exception("Error loading data from the JSONL file.")
            return None

    def save(self, model_input: list | dict, model_output: list) -> None:
        """Save production data to a JSON Lines file.

        If the file doesn't exist, this function will create it.
        """
        logging.info("Storing production data in the JSONL file...")

        # Ensure model_input is a list
        if isinstance(model_input, dict):
            model_input = [model_input]

        try:
            # Process and save each prediction
            current_time = datetime.now(timezone.utc).isoformat()
            
            records = []
            for i, (input_data, output_data) in enumerate(zip(model_input, model_output)):
                record_uuid = str(uuid.uuid4())
                
                # Handle input data
                input_path = input_data.get('file_path')
                if not input_path:
                    # If no file path was provided, save the input data to a file
                    if 'data' in input_data and isinstance(input_data['data'], np.ndarray):
                        input_path = os.path.join(self.data_dir, f"{record_uuid}_input.tif")
                        tifffile.imwrite(input_path, input_data['data'])
                
                # Handle output data
                output_path = output_data.get('output_path')
                if output_path:
                    # Copy the output file to our data directory for long-term storage
                    new_output_path = os.path.join(self.data_dir, f"{record_uuid}_output.tif")
                    try:
                        # Read and then write the file (to avoid file system issues)
                        output_data_array = tifffile.imread(output_path)
                        tifffile.imwrite(new_output_path, output_data_array)
                        output_path = new_output_path
                    except Exception as e:
                        logging.error(f"Failed to copy output file: {e}")
                
                # Extract N-SSIM if available
                n_ssim = output_data.get('n_ssim')
                
                # Get ground truth path if available
                ground_truth_path = input_data.get('ground_truth_path')
                
                # Create record
                record = {
                    "uuid": record_uuid,
                    "input_path": input_path,
                    "output_path": output_path,
                    "n_ssim": n_ssim,
                    "ground_truth_path": ground_truth_path,
                    "prediction_date": current_time
                }
                
                records.append(record)
            
            # Append to the JSON Lines file
            with open(self.storage_file, "a") as f:
                for record in records:
                    f.write(json.dumps(record) + "\n")
                    
            logging.info(f"Saved {len(records)} prediction records to JSONL file")

        except Exception:
            logging.exception("There was an error saving production data to the JSONL file.")

    def label(self, ground_truth_quality: float = 0.8) -> int:
        """Label unlabeled samples by generating synthetic ground truth."""
        if not Path(self.storage_file).exists():
            logging.error("Storage file %s does not exist.", self.storage_file)
            return 0

        try:
            # Read the existing data
            with open(self.storage_file, "r") as f:
                lines = f.readlines()
            
            records = [json.loads(line) for line in lines]
            
            # Count how many records need labeling
            labeled_count = 0
            
            # Process each record
            for i, record in enumerate(records):
                if record.get("ground_truth_path") is None and record.get("output_path") is not None:
                    uuid_val = record["uuid"]
                    output_path = record["output_path"]
                    
                    if not os.path.exists(output_path):
                        continue
                    
                    try:
                        # Load the prediction
                        prediction = tifffile.imread(output_path)
                        
                        # Generate synthetic ground truth
                        ground_truth = self.get_fake_label(prediction, ground_truth_quality)
                        
                        # Save the synthetic ground truth
                        ground_truth_path = os.path.join(self.data_dir, f"{uuid_val}_ground_truth.tif")
                        tifffile.imwrite(ground_truth_path, ground_truth)
                        
                        # Update the record
                        record["ground_truth_path"] = ground_truth_path
                        records[i] = record
                        
                        labeled_count += 1
                    except Exception as e:
                        logging.error(f"Error labeling sample {uuid_val}: {e}")
            
            # Write back all records if we labeled anything
            if labeled_count > 0:
                with open(self.storage_file, "w") as f:
                    for record in records:
                        f.write(json.dumps(record) + "\n")
            
            logging.info(f"Labeled {labeled_count} samples")
            return labeled_count
            
        except Exception:
            logging.exception("There was an error labeling production data")
            return 0

    def invoke(self, payload: list | dict) -> dict | None:
        """Make a prediction request to the hosted model."""
        import requests

        logging.info('Running prediction on "%s"...', self.target)

        try:
            predictions = requests.post(
                url=self.target,
                headers={"Content-Type": "application/json"},
                data=json.dumps(
                    {
                        "inputs": payload,
                    },
                ),
                timeout=5,
            )
            return predictions.json()
        except Exception:
            logging.exception("There was an error sending traffic to the endpoint.")
            return None

    def deploy(self, model_uri: str, model_version: str) -> None:
        """Not Implemented.

        Deploying a model is not applicable when serving the model directly.
        """
        logging.info(
            "Deploy not implemented for JsonLines backend. "
            "Use 'mlflow models serve' to serve the model."
        )


class _FuseMyCellWrapper:
    """
    A wrapper class that loads artifacts and implements prediction logic for the FuseMyCell model.
    This class is used by MLflow's pyfunc model flavor.
    """
    
    def __init__(self, model_path):
        """
        Load model and artifacts from the given path.
        
        Args:
            model_path: Path to the directory containing model artifacts
        """
        import torch
        import importlib.util
        from pathlib import Path
        
        logging.info(f"Loading FuseMyCell model from {model_path}")
        self.model_path = model_path
        
        # Load model architecture
        model_file = Path(model_path) / "model.pth"
        artifacts_dir = Path(model_path) / "artifacts"
        
        # Import the model definition
        spec = importlib.util.spec_from_file_location(
            "unet3d", 
            Path(model_path) / "code" / "unet3d.py"
        )
        unet_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(unet_module)
        
        # Determine device (CPU or GPU)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logging.info(f"Using device: {self.device}")
        
        # Create model instance
        self.model = unet_module.UNet3D(in_channels=1, out_channels=1, init_features=64)
        
        # Load model weights
        self.model.load_state_dict(torch.load(
            artifacts_dir / "model.pth", 
            map_location=self.device
        ))
        self.model = self.model.to(self.device)
        self.model.eval()
        
        logging.info("FuseMyCell model loaded successfully")
    
    def predict(self, context, model_input):
        """
        Generate fused view predictions from single view inputs.
        
        Args:
            context: MLflow model context
            model_input: Dictionary containing the input data
                {'single_view': numpy array or path to TIFF file}
                
        Returns:
            Dictionary containing the prediction results
        """
        # Process input (which could be a file path or numpy array)
        if isinstance(model_input, dict) and 'single_view' in model_input:
            input_data = model_input['single_view']
        else:
            input_data = model_input
            
        # Handle different input types
        if isinstance(input_data, str):
            # Input is a file path
            logging.info(f"Loading input from file: {input_data}")
            try:
                single_view = tifffile.imread(input_data)
                
                # Handle different dimensions
                if len(single_view.shape) == 2:  # Single 2D image
                    single_view = single_view[np.newaxis, ...]
                elif len(single_view.shape) == 4:  # Multiple channels
                    # For simplicity, just take the first channel
                    single_view = single_view[..., 0]
                
            except Exception as e:
                raise ValueError(f"Error loading input file: {str(e)}")
        elif isinstance(input_data, np.ndarray):
            # Input is a numpy array
            single_view = input_data
        else:
            raise ValueError(f"Unsupported input type: {type(input_data)}")
        
        # Generate prediction
        fused_view = predict_fused_view(self.model, single_view)
        
        # Save prediction to a temporary file if needed
        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:
            output_path = tmp.name
            tifffile.imwrite(output_path, fused_view)
        
        return {
            'fused_view': fused_view,
            'output_path': output_path
        }